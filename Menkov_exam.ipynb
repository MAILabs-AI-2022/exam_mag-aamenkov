{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m_5UK_PGKHH-"
      },
      "outputs": [],
      "source": [
        "def create_dataset():\n",
        "    with open(\"/content/exam/dutch.txt\", 'r') as infile:\n",
        "        dutch = infile.read().split(\"\\n\")\n",
        "    \n",
        "    with open(\"/content/exam/hungarian.txt\", 'r') as infile:\n",
        "        hungarian = infile.read().split(\"\\n\")\n",
        "    \n",
        "    with open(\"/content/exam/portugese.txt\", 'r') as infile:\n",
        "         portugese = infile.read().split(\"\\n\")\n",
        "    \n",
        "    dutch_count = int(0.8*len(dutch))\n",
        "    dutch_train = dutch[ : dutch_count]\n",
        "    dutch_test = dutch[dutch_count:]\n",
        "\n",
        "    hungarian_count = int(0.8*len(dutch))\n",
        "    hungarian_train = dutch[ : dutch_count]\n",
        "    hungarian_test = dutch[dutch_count:]\n",
        "\n",
        "    portugese_count = int(0.8*len(dutch))\n",
        "    portugese_train = dutch[ : dutch_count]\n",
        "    portugese_test = dutch[dutch_count:]\n",
        "\n",
        "    return dutch_train, hungarian_train, portugese_train, dutch_test, hungarian_test, portugese_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dutch_train, hungarian_train, portugese_train, dutch_test, hungarian_test, portugese_test = create_dataset()  "
      ],
      "metadata": {
        "id": "WhyS8boRKNmj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = [\"UNK\"] + list(set(dutch_train + hungarian_train + portugese_train))\n",
        "word_to_index = { item : index for index, item in enumerate(index_to_word) }"
      ],
      "metadata": {
        "id": "xRB5YXLIKO7z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, embedding_input: int):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(embedding_input, 128)\n",
        "        self.linear_1 = nn.Linear(128, 32)\n",
        "        self.linear_2 = nn.Linear(32, 3)\n",
        "        \n",
        "    def forward(self, token):\n",
        "        relu = nn.ReLU()\n",
        "        output = self.embedding(token)\n",
        "        output = relu(output)\n",
        "        output = self.linear_1(output)\n",
        "        output = relu(output)\n",
        "        output = self.linear_2(output)\n",
        "        output = nn.Sigmoid()(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "FDqmCMloK-Pl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dutch, hungarian, portugese):    \n",
        "        super().__init__()\n",
        "        self.data = []\n",
        "        self.target = []\n",
        "\n",
        "        for target_index, tokens_list in enumerate([dutch, hungarian, portugese]):\n",
        "            for token in tokens_list:\n",
        "                self.data.append(word_to_index.get(token, 0))\n",
        "                self.target.append(target_index)\n",
        "        self.data = torch.tensor(self.data)\n",
        "        self.target = torch.tensor(self.target) \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.target[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target.shape[0]   "
      ],
      "metadata": {
        "id": "SHM38xWiL0s0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader  \n",
        "\n",
        "train_dataset = CustomDataset(dutch_train, hungarian_train, portugese_train)\n",
        "test_dataset = CustomDataset(dutch_train, hungarian_train, portugese_train)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "mlsVOp9FTPuD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_net = Net(len(index_to_word))\n",
        "optimizer = torch.optim.Adam(neural_net.parameters())\n",
        "entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    train_history = []\n",
        "    test_history = []\n",
        "    neural_net.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data, target = batch \n",
        "        prediction = neural_net(data)\n",
        "        loss = entropy(prediction, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_history.append(loss.item())\n",
        "    neural_net.eval()\n",
        "    for batch in test_loader:\n",
        "        data, target = batch \n",
        "        prediction = neural_net(data)\n",
        "        loss = entropy(prediction, target)\n",
        "        test_history.append(loss.item())\n",
        "    print(  f\"Epoch {epoch} train history {sum(train_history) / len(train_history)} test history {sum(test_history)/ len(test_history)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "933qkS6QNzWh",
        "outputId": "d5278811-ccbc-4052-a9c3-fa1591fd9162"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 train history 0.9390934077051256 test history 1.2171184798482515\n",
            "Epoch 1 train history 1.1887595976464 test history 1.2137234031467\n",
            "Epoch 2 train history 1.166942331564186 test history 1.2172166357294627\n",
            "Epoch 3 train history 1.2989629356764922 test history 1.1949750398009242\n",
            "Epoch 4 train history 1.2470451705909256 test history 1.1949286786383098\n",
            "Epoch 5 train history 1.1949639881206864 test history 1.1948041146358084\n",
            "Epoch 6 train history 1.3131770920169457 test history 1.0986123026619727\n",
            "Epoch 7 train history 1.0986264746539531 test history 1.0986123162319061\n",
            "Epoch 8 train history 1.0986360618978825 test history 1.0986123244769292\n",
            "Epoch 9 train history 1.0986467548337382 test history 1.0986122963064342\n"
          ]
        }
      ]
    }
  ]
}